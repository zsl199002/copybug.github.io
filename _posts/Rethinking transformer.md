

### Introduction
---
### Multi-Head Self-Attention

this is the most important parts of the transformer

### Current application cases

### What's the intuition behind multi-head?

To current understanding, transformer performs better in **learning long-range dependencies**,then why

Q: what context I need as attribute A

K: attribute A

v: the meaning as attribute A

### Refrence

[1] [Seq2seq pay Attention to Self Attention: Part 2](https://medium.com/@bgg/seq2seq-pay-attention-to-self-attention-part-2-cf81bf32c73d)
